{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Twitter.ipynb","version":"0.3.2","provenance":[{"file_id":"1BptE5P2wrW39MZlG7V-6KyND6W9PVTRA","timestamp":1566828404295},{"file_id":"1jMQdFloRMyYQ50C-FBXWwwe1_TdV8dzb","timestamp":1564299258808},{"file_id":"1MSajMQzhQX54KrTqcYcPJMw7jic5Gv2w","timestamp":1564137071134},{"file_id":"16vZ7JUQTybAM-JMMu8AXglfAJ81etkpI","timestamp":1564128540852},{"file_id":"1gPHfc6GDalt-dp0rWHEGbDTpUAJemBu9","timestamp":1563263251509}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4tbSJyG4K_gx","colab_type":"text"},"source":["# Sentiment Analysis\n","***\n","Sentiment Analysis is a field within Natural Language Processing (NLP) that builds systems that try to identify and extract opinions within text. So it tries to determine whether a piece of text is positive, neutral or negative and this is done through the use of variables such as context , tone and emotion.\n"]},{"cell_type":"markdown","metadata":{"id":"zniAWNcYOGj7","colab_type":"text"},"source":["# Sentiment Analysis using the Twitter dataset\n","***\n","Through out this tutorial we are going to try and classify tweets into positive or negative sentiment based on their content.\n","\n","### Getting started\n","To get started we need to import some important modules to help us:\n","\n","1. Keras  -  A high-level neural networks API, written in Python.\n","\n","2. Matplotlib - 2D plotting library.\n","\n","3. Pandas - library providing high-performance, easy-to-use data structures and data analysis tools.\n","\n","4. Seaborn  - module implements a fundamental, but powerful algorithm for serializing and de-serializing a Python object structure.\n","\n","5. SkLearn - Simple and efficient tools for data mining and data analysis\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uRax_bjTPApI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6c4a461f-7041-46b9-e0b0-b6d03ba1d562","executionInfo":{"status":"ok","timestamp":1566828781633,"user_tz":-60,"elapsed":3068,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}}},"source":["import keras\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, MaxPooling1D, Activation, Flatten, SpatialDropout1D\n","from keras.optimizers import SGD\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","\n","import seaborn as sns\n","import re\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import SGDClassifier"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"u2AwWRrhO_6x","colab_type":"text"},"source":["### Load and Clean the Data\n","Load the data(train.csv) as before with pandas and take a look at the data so we know what we will be working with.\n","\n","To Do:\n","\n","- Load the dataset into a dataframe with pandas\n","- Clean the data so it is easier to work with."]},{"cell_type":"code","metadata":{"id":"5Rh-qfYQxY9p","colab_type":"code","outputId":"c3f3b9a8-2c40-45a6-b2fe-f17cc7c2bf48","executionInfo":{"status":"ok","timestamp":1566828821439,"user_tz":-60,"elapsed":4391,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#load the train.csv file to a dataframe\n","tweets = pd.read_csv('https://ai-camp-content.s3.amazonaws.com/train.csv', ',', encoding='ISO-8859-1')\n","\n","#Display the first 5 rows of the dataframe\n","tweets.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ItemID</th>\n","      <th>Sentiment</th>\n","      <th>SentimentText</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>is so sad for my APL frie...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>I missed the New Moon trail...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>omg its already 7:30 :O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>i think mi bf is cheating on me!!!   ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ItemID  Sentiment                                      SentimentText\n","0       1          0                       is so sad for my APL frie...\n","1       2          0                     I missed the New Moon trail...\n","2       3          1                            omg its already 7:30 :O\n","3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n","4       5          0           i think mi bf is cheating on me!!!   ..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"n1B4mpGmRcNR","colab_type":"code","outputId":"e2578545-6457-4077-b20b-f84eee4675c9","executionInfo":{"status":"ok","timestamp":1566828952932,"user_tz":-60,"elapsed":1211,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#View the shape of the dataframe\n","tweets.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(99989, 3)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"bixgl2nnxh-G","colab_type":"code","colab":{}},"source":["#clean up the tweets\n","\n","#Make the text lowercase\n","tweets['SentimentText'] = tweets['SentimentText'].apply(lambda x: x.lower())\n","\n","#Replace any special characters\n","tweets['SentimentText'] = tweets['SentimentText'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGlCTZ7Bxl0z","colab_type":"code","outputId":"88d7acc1-1515-4d04-e5a7-44e30bda7426","executionInfo":{"status":"ok","timestamp":1566828964323,"user_tz":-60,"elapsed":523,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["tweets.head(5)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ItemID</th>\n","      <th>Sentiment</th>\n","      <th>SentimentText</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>is so sad for my apl friend</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>i missed the new moon trailer</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>omg its already 730 o</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>omgaga im sooo  im gunna cry ive be...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>i think mi bf is cheating on me      ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ItemID  Sentiment                                      SentimentText\n","0       1          0                        is so sad for my apl friend\n","1       2          0                      i missed the new moon trailer\n","2       3          1                              omg its already 730 o\n","3       4          0             omgaga im sooo  im gunna cry ive be...\n","4       5          0           i think mi bf is cheating on me      ..."]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"9QQYtKVmxuOY","colab_type":"code","outputId":"92a29e2b-b651-4f63-b4a0-c95667909d2b","executionInfo":{"status":"ok","timestamp":1566829112118,"user_tz":-60,"elapsed":1247,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":300}},"source":["#Plot the sentiment on a graph using sns\n","tweets['Sentiment'].value_counts()\n","sns.countplot(tweets['Sentiment'])"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f2c9e2b76d8>"]},"metadata":{"tags":[]},"execution_count":9},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"q66LRMr75TPH","colab_type":"text"},"source":["### Tokenise\n","We are going to use tokenisation to find the 2000 most common words. \n","\n","A token is an instance of a sequence of characters. A type is the class of all tokens containing the same character sequence.\n","\n","To Do:\n","\n","- Define the max features\n","- Initialise a tokeniser with the max feature and a defined split\n","- Calculate the different word frequencies\n","- Add padding to the sequences\n","\n","\n","Replace the \"...\" in the following code to make it run!"]},{"cell_type":"code","metadata":{"id":"lhrbl-Kz5S_B","colab_type":"code","outputId":"3e744db0-25ff-43c5-8a2f-36072eb6ed8e","executionInfo":{"status":"ok","timestamp":1566829553522,"user_tz":-60,"elapsed":5158,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#tokenisation\n","\n","max_features = 2000\n","\n","#create a tokenizer that takes the 2000 most common words and Separator for word splitting.\n","tokenizer = Tokenizer(num_words=max_features , split=' ') #The space is the separator \n","\n","#calculate the frequency of each word in the dataset - fit_on_texts creates the vocabulary index based on word frequency. Every word gets its a unique innteger value and 0 is reserved for padding.\n","tokenizer.fit_on_texts(tweets['SentimentText'].values)\n","\n","#text_to-sequence basically takes each word in the text and replaces it with its corresponding integer value.\n","X = tokenizer.texts_to_sequences(tweets['SentimentText'].values)\n","\n","#applying padding as the NN needs samples that are the same size\n","X = pad_sequences(X)\n","print(X[:1])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0  10  18 123   9   8 266]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vx_oNb2p07eJ","colab_type":"text"},"source":["## Build the Model\n","As before define and build the model you want to train with this dataset.\n","\n","To Do:\n","\n","- Define a sequential model and add your layers.\n","- Compile the model\n","- (Optional) Display the layers with summary()\n","\n","\n","\n","Replace the \"...\" in the following code to make it run!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dUH-2qw_06Kp","outputId":"98fc9673-e42f-4014-a24a-5065cba55cb5","executionInfo":{"status":"ok","timestamp":1566832325380,"user_tz":-60,"elapsed":1122,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["#larger lstm dropout + lstm_out = embed_dim + extra layers.\n","#Note that embed_dim and lstm_out are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. \n","embed_dim = 128\n","lstm_out = 196\n","\n","\n","#A Sequential model is a linear stack of layers. Intialise an empty sequential model to add layers to \n","model = Sequential()\n","\n","#embedding layer lets the network expand each token into a larger vector\n","#Fill in the following parameters: 1. size of our vocabulary, 2. embedding dimesion(embed_dim) - expands to a vector of 128 3. length of input\n","model.add(Embedding(max_features, embed_dim,  input_length = X.shape[1]))\n","\n","#SpatialDropout1D takes a paramter of a float between 0 and 1. The fraction of the input units to drop.\n","model.add(SpatialDropout1D(0.1))\n","\n","#CNN layer gives a higher view of the sequences to the LSTM example: \"I loved this friendly service\"\n","#could be processed as \"I love this\" \"Friendly service\" which is two chunks for the LSTM rather than 5 chunks\n","\n","\n","#1. the size of our word embeddings, 2&3. dropouts - resetting a random amount of weights (make it harder for the \n","#NN to learn patterns)\n","model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","\n","\n","#Dense is a regular densely-connected NN layer, it takes in units which is a positive integer, dimensionality of the output space and the Activation function to use.\n","model.add(Dense(128, activation=\"tanh\"))\n","model.add(Dense(2, activation=\"softmax\"))\n","\n","# compiles the model using backend library (tensorflow), using categorical becuase our targets are one-hot encoded.\n","#adam is an algorithm for first-order gradient-based optimization\n","#metrics is a list of metrics to be evaluated by the model during training and testing\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n","\n","print(model.summary())"],"execution_count":35,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_7 (Embedding)      (None, 84, 128)           256000    \n","_________________________________________________________________\n","spatial_dropout1d_5 (Spatial (None, 84, 128)           0         \n","_________________________________________________________________\n","lstm_5 (LSTM)                (None, 196)               254800    \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 128)               25216     \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 2)                 258       \n","=================================================================\n","Total params: 536,274\n","Trainable params: 536,274\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ik5SxCIVS9_Y","colab_type":"text"},"source":["## Split the Data and Train\n","\n","Again we need to split the data so we have testing data and validation data to work with. The easiest way to do this is to make use of the scikit-learn library's function train_test_split.\n","\n","To Do:\n","\n","- Train the model with the data you have just organised\n","- Make predictions \n","\n","Replace the \"...\" in the following code to make it run"]},{"cell_type":"code","metadata":{"id":"MQDYqBUyTGbt","colab_type":"code","outputId":"7cc37418-4951-4ec8-a1ee-162377a95923","executionInfo":{"status":"ok","timestamp":1566832444552,"user_tz":-60,"elapsed":1358,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#get_dummies converts categorical variables into dunny/indicator variables\n","Y  = pd.get_dummies(tweets['Sentiment']).values\n","\n","#Use train_test_split to split arrays or matrices into random train and test subsets\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n","\n","#Print out the shape for X and Y both train and test\n","print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["(79991, 84) (19998, 84) (79991, 2) (19998, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZsnHgfW11otf","colab_type":"code","outputId":"986d7da1-84a4-46a3-82a0-d13038573945","executionInfo":{"status":"ok","timestamp":1566832677567,"user_tz":-60,"elapsed":209994,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# The batch size defines the number of samples that will be propagated through the network.\n","batch_size  = 500\n","\n","#Model.fit - Trains the model for a given number of iterations on a dataset.\n","#Parameters - (X and y train, batch_size, iterations, validation data(e.g. X and Y test))\n","model.fit(x_train, \n","          y_train, \n","          batch_size=batch_size,\n","          epochs = 5,\n","          validation_data=(x_test, y_test))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Train on 79991 samples, validate on 19998 samples\n","Epoch 1/5\n","79991/79991 [==============================] - 42s 524us/step - loss: 0.5017 - acc: 0.7552 - val_loss: 0.5041 - val_acc: 0.7559\n","Epoch 2/5\n","79991/79991 [==============================] - 42s 522us/step - loss: 0.4815 - acc: 0.7677 - val_loss: 0.4961 - val_acc: 0.7613\n","Epoch 3/5\n","79991/79991 [==============================] - 42s 521us/step - loss: 0.4697 - acc: 0.7725 - val_loss: 0.4904 - val_acc: 0.7627\n","Epoch 4/5\n","79991/79991 [==============================] - 42s 522us/step - loss: 0.4609 - acc: 0.7793 - val_loss: 0.4895 - val_acc: 0.7631\n","Epoch 5/5\n","79991/79991 [==============================] - 42s 522us/step - loss: 0.4521 - acc: 0.7829 - val_loss: 0.4866 - val_acc: 0.7628\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f2c4bccb048>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"cv-bmGJErn-l","colab_type":"code","colab":{}},"source":["#Make predictions\n","predictions = model.predict_classes(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QZ4YS3Sur470","colab_type":"code","outputId":"22f39f62-2a98-44c8-d44e-2d93ea2d1204","executionInfo":{"status":"ok","timestamp":1566831802390,"user_tz":-60,"elapsed":1703,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["predictions"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 0])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"YJfEpv_BtLjF","colab_type":"code","colab":{}},"source":["#convert the tokenized sequences of X test, back into text to display it\n","X_text = tokenizer.sequences_to_texts(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"07880cdd-e8cd-4d41-ed82-bc1f3558b698","executionInfo":{"status":"ok","timestamp":1566831865600,"user_tz":-60,"elapsed":1314,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"id":"eOFeLz5jtKvH","colab":{"base_uri":"https://localhost:8080/","height":359}},"source":["#Display predictions along side tweets\n","d = {'Prediction': predictions, 'Tweet': X_text}\n","df = pd.DataFrame(d)\n","pd.set_option('max_colwidth', 1300)\n","display(df.head(10))"],"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Prediction</th>\n","      <th>Tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>bass you never say that to me</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>amp me made me and i was so sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>i do dogs around and cats too something about me and</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>ashleytisdale please follow me im following you</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>amp c a none worked my poor are now</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>head all well</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>to v word a live u ma to</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>hope the weather has</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>my new for review today that was fast me and happy thanks</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Prediction                                                      Tweet\n","0           1                              bass you never say that to me\n","1           0                            amp me made me and i was so sad\n","2           1                                                           \n","3           0       i do dogs around and cats too something about me and\n","4           1            ashleytisdale please follow me im following you\n","5           0                        amp c a none worked my poor are now\n","6           1                                              head all well\n","7           1                                   to v word a live u ma to\n","8           0                                       hope the weather has\n","9           1  my new for review today that was fast me and happy thanks"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"m3U0xXkGvHaG","colab_type":"code","outputId":"c1ce1cbd-b5a5-45fa-b76e-ab3aafcf5299","executionInfo":{"status":"ok","timestamp":1566832825757,"user_tz":-60,"elapsed":45350,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#Show the accuracy\n","score, accuracy = model.evaluate(x_test, y_test)\n","\n","print('Test Score', score)\n","print('Test accuracy:', accuracy)\n"],"execution_count":41,"outputs":[{"output_type":"stream","text":["19998/19998 [==============================] - 45s 2ms/step\n","Test Score 0.4865673397741195\n","Test accuracy: 0.7627762776396849\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LsWWi2sJPmak","colab_type":"text"},"source":["## Exercises\n","\n","1. Try to impove the models accuracy \n"," \n"," Hint:\n","    - Try changing things like embed_dim, lstm_out, epochs and dropout\n","      \n","      \n","      "]},{"cell_type":"code","metadata":{"id":"SpNA5McZCC9C","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGmY1y5mCHTw","colab_type":"text"},"source":["# Stretch Exercise:\n","\n","In the above tutorial we created our model using Keras - For this exercise, we would like you to complete the same challenge but use the SKLearn SDGClassifier instead. \n","\n","The goal of this exercise is to show you ways of doing the same thing using different libraries."]},{"cell_type":"markdown","metadata":{"id":"ULnviGQNCDOB","colab_type":"text"},"source":["## Split the Data and Train\n","The twitter dataset should already be cleaned from above - so we can go straight to spliting it up using the scikit-learn library's function train_test_split."]},{"cell_type":"code","metadata":{"id":"iwMLQvszCMNv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"fed9957d-4cd6-4511-fdff-c228b50cb8d9","executionInfo":{"status":"ok","timestamp":1566833614970,"user_tz":-60,"elapsed":1396,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}}},"source":["#Use train_test_split to split arrays or matrices into random train and test subsets\n","x_train, x_test, y_train, y_test = train_test_split(tweets['SentimentText'], tweets['Sentiment'], test_size = 0.2)\n","\n","#Print out the shape for X and Y both train and test\n","print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n","\n","#Print out the first 10 values in Y\n","print(Y[:10])"],"execution_count":48,"outputs":[{"output_type":"stream","text":["(79991,) (19998,) (79991,) (19998,)\n","[[1 0]\n"," [1 0]\n"," [0 1]\n"," [1 0]\n"," [1 0]\n"," [1 0]\n"," [0 1]\n"," [1 0]\n"," [0 1]\n"," [0 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8HrKHECkCrxd","colab_type":"text"},"source":["## Count Vectorizer\n","\n","Previously in the tutorial we used the Keras Tokenizer to find character sequences.\n","\n","However, in SKLearn we use a CountVectorizer instead.\n","\n","The CountVectorizer can Convert a collection of text documents to a matrix of token counts\n"]},{"cell_type":"code","metadata":{"id":"Om_IfPuqCMVO","colab_type":"code","colab":{}},"source":["#Create CountVectorizer\n","CV = CountVectorizer\n","\n","#Fit and then transform it with the Xtrain data. (e.g. fit_transform())\n","X_Train_counts = "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMdsMxjcH2TT","colab_type":"text"},"source":["## SGDClassifer\n","\n","This estimator implements regularized linear models with stochastic gradient descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate)"]},{"cell_type":"code","metadata":{"id":"01wJ8PHLCMa2","colab_type":"code","colab":{}},"source":["#Create the SGDClassifer - it takes two paramters: 1. Max iterations and 2. total\n","clf = ...\n","\n","#Fit the SGDClassifer with the X_Train counts and the Y train\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"63tlBwYPCMel","colab_type":"code","colab":{}},"source":["#Create predictions by passing in a transformed version of X test\n","predictions = clf.predict(...)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuGnNJ6SCUlI","colab_type":"code","colab":{}},"source":["#Display the predictions \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nuF2K0OCUqp","colab_type":"code","colab":{}},"source":["#Show the accuracy by using the metrics.accuracy_score function\n"],"execution_count":0,"outputs":[]}]}