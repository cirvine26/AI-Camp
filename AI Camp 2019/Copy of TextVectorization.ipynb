{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of TextVectorization.ipynb","version":"0.3.2","provenance":[{"file_id":"105Y4-U3WseiL-3YdgcCHN17F66GrR8gL","timestamp":1566815855846},{"file_id":"1w8H7zOZ4ElqG3JwKlGjPwWFqp3WJWWRT","timestamp":1564756142090}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"UWDSw_IGpEy5","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n","from sklearn.preprocessing import Binarizer\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from __future__ import print_function\n","\n","import numpy as np "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwzVQibbyhdo","colab_type":"text"},"source":["#Text Vectorization\n","\n","In this exercise we will explore the different forms of encoding our text so that it can be used in a machine learning model.  First we must take our full text and split it into 'documents'.  In the example below we are spliting it into different sentences.\n","\n","*Note: A text corpus is a large and unstructured set of texts*"]},{"cell_type":"code","metadata":{"id":"URXGk8UqpCwL","colab_type":"code","colab":{}},"source":["text = \"\"\"There are a number of ways you can organise the data so we can train a classifier on it. A TfidfVectorizer or a CountVectorizer are both good for the job since we are working with large amounts of text. A TfidfVectorizer does what a CountVectorizer does with a TfidfTransformer on top.\"\"\"\n","corpus = text.split('. ')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOwKfVgQ0ZVf","colab_type":"text"},"source":["# Keras Implementation\n","## Tokenization\n","\n","We can use the [keras Tokenizer](http://faroit.com/keras-docs/2.0.6/preprocessing/text/) to tokenize all of the text.  By default this splits up all text into word tokens.  As you can see below, we don't pass in any parameters, leaving all defaults as they are.  Have a look at the official documentation to see what other parameters are set by default."]},{"cell_type":"code","metadata":{"id":"LcazeHLQ8wPY","colab_type":"code","colab":{}},"source":["t = Tokenizer()\n","t.fit_on_texts(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeuETiiy1AyS","colab_type":"text"},"source":["Once the tokens have been created, we have access to them using some of the following attributes.  Based on the output and the names of them, what do you think they are giving us?\n","\n","*Hint: Not sure? Check the official documentation!*"]},{"cell_type":"code","metadata":{"id":"4cx3kB899A2C","colab_type":"code","outputId":"e52b15ae-895c-4aeb-c299-9c076e1503df","executionInfo":{"status":"ok","timestamp":1566816121932,"user_tz":-60,"elapsed":714,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":142}},"source":["total_words = len(t.word_index)+1\n","print(total_words)\n","print(t.word_counts)\n","print(t.document_count)\n","print(t.word_index)\n","print(t.word_docs)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["35\n","OrderedDict([('there', 1), ('are', 3), ('a', 7), ('number', 1), ('of', 2), ('ways', 1), ('you', 1), ('can', 2), ('organise', 1), ('the', 2), ('data', 1), ('so', 1), ('we', 2), ('train', 1), ('classifier', 1), ('on', 2), ('it', 1), ('tfidfvectorizer', 2), ('or', 1), ('countvectorizer', 2), ('both', 1), ('good', 1), ('for', 1), ('job', 1), ('since', 1), ('working', 1), ('with', 2), ('large', 1), ('amounts', 1), ('text', 1), ('does', 2), ('what', 1), ('tfidftransformer', 1), ('top', 1)])\n","3\n","{'a': 1, 'are': 2, 'of': 3, 'can': 4, 'the': 5, 'we': 6, 'on': 7, 'tfidfvectorizer': 8, 'countvectorizer': 9, 'with': 10, 'does': 11, 'there': 12, 'number': 13, 'ways': 14, 'you': 15, 'organise': 16, 'data': 17, 'so': 18, 'train': 19, 'classifier': 20, 'it': 21, 'or': 22, 'both': 23, 'good': 24, 'for': 25, 'job': 26, 'since': 27, 'working': 28, 'large': 29, 'amounts': 30, 'text': 31, 'what': 32, 'tfidftransformer': 33, 'top': 34}\n","defaultdict(<class 'int'>, {'the': 2, 'are': 2, 'we': 2, 'a': 3, 'organise': 1, 'data': 1, 'on': 2, 'it': 1, 'you': 1, 'classifier': 1, 'ways': 1, 'can': 1, 'train': 1, 'so': 1, 'there': 1, 'of': 2, 'number': 1, 'with': 2, 'for': 1, 'amounts': 1, 'good': 1, 'countvectorizer': 2, 'or': 1, 'job': 1, 'working': 1, 'both': 1, 'large': 1, 'text': 1, 'tfidfvectorizer': 2, 'since': 1, 'tfidftransformer': 1, 'top': 1, 'does': 1, 'what': 1})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XoHp-NfMsc0y","colab_type":"text"},"source":["## Keras Sequence\n","\n","The default way that Keras produces vectors that we may use in a machine learning is by converting the text to sequences.  These are a list of word indexes relating to the dictionary created on `fit_on_texts`.\n","\n","###Question\n","Have a look at the output of one of the tokenizer attributes.  Can you manually uncode the first 3 words of any of the three documents?"]},{"cell_type":"code","metadata":{"id":"6iTbPKQOpMXs","colab_type":"code","outputId":"3d1789db-5408-47b8-cfd5-44a6d659549c","executionInfo":{"status":"ok","timestamp":1566816163504,"user_tz":-60,"elapsed":482,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X = t.texts_to_sequences(corpus)\n","X"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4, 19, 1, 20, 7, 21],\n"," [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28, 10, 29, 30, 3, 31],\n"," [1, 8, 11, 32, 1, 9, 11, 10, 1, 33, 7, 34]]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"bI5QFapMvO9k","colab_type":"text"},"source":["An important point to reiterate is that most machine learning algorithms require each observation/feature vector to be the same size.  However as we can see from the output above, each of our 3 documents is a different size because each document has a different word count.  We can fix this by appling padding.\n","\n","###Question\n","Where is the padding applied and if the code were to be changed to apply it elsewhere, what would need added to the below statement?"]},{"cell_type":"code","metadata":{"id":"vRNAr6ZDqAJS","colab_type":"code","outputId":"ad5eaff1-1907-4aed-fc2d-61aa62d98d0b","executionInfo":{"status":"ok","timestamp":1566816165170,"user_tz":-60,"elapsed":482,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["pad_sequences(X)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0, 12,  2,  1, 13,  3, 14, 15,  4, 16,  5, 17, 18,  6,  4, 19,\n","         1, 20,  7, 21],\n","       [ 1,  8, 22,  1,  9,  2, 23, 24, 25,  5, 26, 27,  6,  2, 28, 10,\n","        29, 30,  3, 31],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  8, 11, 32,  1,  9, 11, 10,\n","         1, 33,  7, 34]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"cF9h6ITpsTEl","colab_type":"code","outputId":"4097810a-b928-46f4-8cc1-33a905d83ea4","executionInfo":{"status":"ok","timestamp":1566816169644,"user_tz":-60,"elapsed":483,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pad_sequences(X).shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 20)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"yHj4lBYWVQFK","colab_type":"text"},"source":["As you can see, the tokenizer also performs some word counting and therefore could be used to create a frequency vector.  Be careful however! There is a difference between the above output and the following encodings.\n","\n","## Texts to Matrix\n","\n","The `texts_to_matrix` function is the way Keras encodes documents to vectors.  There is a simple mode  parameter that can be changed to cover all options covered so far."]},{"cell_type":"code","metadata":{"id":"T5ZtAXVRVkXD","colab_type":"code","outputId":"4ccbf3bf-1a81-43f4-ce05-81c9ce503388","executionInfo":{"status":"ok","timestamp":1566816172994,"user_tz":-60,"elapsed":489,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["t = Tokenizer()\n","t.fit_on_texts(corpus)\n","t.texts_to_matrix(corpus, mode=\"binary\").shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 35)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"apaVdZ9nXkp5","colab_type":"text"},"source":["### Question\n","\n","Can you explain the reason why this output is a different shape to the padded sequence above?"]},{"cell_type":"code","metadata":{"id":"EEkCoo9YXf1l","colab_type":"code","outputId":"1eb2c8b0-01f4-48d8-da28-7aba1aa51736","executionInfo":{"status":"ok","timestamp":1566816175740,"user_tz":-60,"elapsed":481,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["t.texts_to_matrix(corpus, mode=\"binary\")"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0.],\n","       [0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"K1udWjnCV9-F","colab_type":"code","outputId":"6dd72c1d-e6ed-4673-e6d5-5e005c00783e","executionInfo":{"status":"ok","timestamp":1566816178324,"user_tz":-60,"elapsed":487,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["t.texts_to_matrix(corpus, mode=\"count\")"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 2., 1., 1., 2., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0.],\n","       [0., 2., 2., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        0., 0., 0.],\n","       [0., 3., 0., 0., 0., 0., 0., 1., 1., 1., 1., 2., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"yGc9JLEkV_ZK","colab_type":"code","outputId":"aae2786b-5021-477e-e185-7172dda80d39","executionInfo":{"status":"ok","timestamp":1566816180262,"user_tz":-60,"elapsed":519,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["t.texts_to_matrix(corpus, mode=\"tfidf\")"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.94751189, 0.69314718, 0.69314718, 1.55141507,\n","        0.69314718, 0.69314718, 0.69314718, 0.        , 0.        ,\n","        0.        , 0.        , 0.91629073, 0.91629073, 0.91629073,\n","        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n","        0.91629073, 0.91629073, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.94751189, 1.17360019, 0.69314718, 0.        ,\n","        0.69314718, 0.69314718, 0.        , 0.69314718, 0.69314718,\n","        0.69314718, 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.91629073, 0.91629073, 0.91629073,\n","        0.91629073, 0.91629073, 0.91629073, 0.91629073, 0.91629073,\n","        0.91629073, 0.91629073, 0.        , 0.        , 0.        ],\n","       [0.        , 1.17441657, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.69314718, 0.69314718, 0.69314718,\n","        0.69314718, 1.55141507, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.91629073, 0.91629073, 0.91629073]])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"Vzub3M4fW03P","colab_type":"text"},"source":["## Question\n","\n","There is another mode that Keras accepts.  What is it and can you explain it's output in the context of our corpus?"]},{"cell_type":"code","metadata":{"id":"urnkm12snpai","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"facdb87b-7572-4130-c991-64cd3c175827","executionInfo":{"status":"ok","timestamp":1566816321160,"user_tz":-60,"elapsed":468,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}}},"source":["# TODO:\n","t.texts_to_matrix(corpus, mode='freq')"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.10526316, 0.05263158, 0.05263158, 0.10526316,\n","        0.05263158, 0.05263158, 0.05263158, 0.        , 0.        ,\n","        0.        , 0.        , 0.05263158, 0.05263158, 0.05263158,\n","        0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158,\n","        0.05263158, 0.05263158, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.1       , 0.1       , 0.05      , 0.        ,\n","        0.05      , 0.05      , 0.        , 0.05      , 0.05      ,\n","        0.05      , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.05      , 0.05      , 0.05      ,\n","        0.05      , 0.05      , 0.05      , 0.05      , 0.05      ,\n","        0.05      , 0.05      , 0.        , 0.        , 0.        ],\n","       [0.        , 0.25      , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.08333333, 0.08333333, 0.08333333,\n","        0.08333333, 0.16666667, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.08333333, 0.08333333, 0.08333333]])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"eyDZUWIF13En","colab_type":"text"},"source":["# Scikit-learn Implementation\n","\n","##Frequency Vectors\n","\n","Now we are going to create an encoded frequency vector using scikit-learns [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  Run the code below and see if can you verify the outputs?  I.e count the number of times the word 'are' appears in each of the sentences in the text and see if you get the same values in the output below."]},{"cell_type":"code","metadata":{"id":"p8rh0ghm9ScL","colab_type":"code","outputId":"8c2b4432-7287-4eab-8bb9-cd3466dd76a1","executionInfo":{"status":"ok","timestamp":1566816330257,"user_tz":-60,"elapsed":552,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","print(vectorizer.get_feature_names())\n","print(X.toarray())"],"execution_count":18,"outputs":[{"output_type":"stream","text":["['amounts', 'are', 'both', 'can', 'classifier', 'countvectorizer', 'data', 'does', 'for', 'good', 'it', 'job', 'large', 'number', 'of', 'on', 'or', 'organise', 'since', 'so', 'text', 'tfidftransformer', 'tfidfvectorizer', 'the', 'there', 'top', 'train', 'ways', 'we', 'what', 'with', 'working', 'you']\n","[[0 1 0 2 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1]\n"," [1 2 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0]\n"," [0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qy-jpgQvri2G","colab_type":"code","outputId":"960144c6-56d8-45cf-b3da-1e6a695daa8f","executionInfo":{"status":"ok","timestamp":1566816335627,"user_tz":-60,"elapsed":467,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.shape"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 33)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"AELksOIysO5h","colab_type":"text"},"source":["### Question\n","\n","Note the difference in shape and values of the frequency and sequence vectors.  Can you explain this? (Hint: Are there any words missing?)"]},{"cell_type":"markdown","metadata":{"id":"tdQGsXMk7IVm","colab_type":"text"},"source":["## One-Hot Vectors\n","\n","Another scheme discussed is the One-Hot encoding scheme which reduces the imbalance due to the distribution of tokens.  An easy way to produce this is to simply take the output from the CountVectorizer and reduce all positive numbers down to a maximum of 1, therefore representing if a word is present at all in a document.  We can do this with the [Binarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html) class from scikit-learn."]},{"cell_type":"code","metadata":{"id":"IWUpXUBQfJVG","colab_type":"code","outputId":"90634c0c-68e6-4814-e089-ab355e513027","executionInfo":{"status":"ok","timestamp":1566816344127,"user_tz":-60,"elapsed":557,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","print(vectorizer.get_feature_names())\n","\n","onehot = Binarizer()\n","X = onehot.fit_transform(X)\n","print(X.toarray())"],"execution_count":20,"outputs":[{"output_type":"stream","text":["['amounts', 'are', 'both', 'can', 'classifier', 'countvectorizer', 'data', 'does', 'for', 'good', 'it', 'job', 'large', 'number', 'of', 'on', 'or', 'organise', 'since', 'so', 'text', 'tfidftransformer', 'tfidfvectorizer', 'the', 'there', 'top', 'train', 'ways', 'we', 'what', 'with', 'working', 'you']\n","[[0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1]\n"," [1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0]\n"," [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5b94WqB6ZJsr","colab_type":"text"},"source":["A much easier solution is to set a particular parameter in the CountVectorizer object creation.  Look up the official documentation to find out what it is and make sure you get the same values."]},{"cell_type":"code","metadata":{"id":"2O3kPx0kYtD8","colab_type":"code","outputId":"0447f950-df6f-4ecb-b097-92b703b1568a","executionInfo":{"status":"ok","timestamp":1566817253890,"user_tz":-60,"elapsed":393,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# TODO: One-Hot encode corpus with CountVectorizer\n","onehot = CountVectorizer()\n","X = onehot.fit_transform(corpus)\n","\n","print(onehot.get_feature_names())\n","print(X.toarray())"],"execution_count":34,"outputs":[{"output_type":"stream","text":["['amounts', 'are', 'both', 'can', 'classifier', 'countvectorizer', 'data', 'does', 'for', 'good', 'it', 'job', 'large', 'number', 'of', 'on', 'or', 'organise', 'since', 'so', 'text', 'tfidftransformer', 'tfidfvectorizer', 'the', 'there', 'top', 'train', 'ways', 'we', 'what', 'with', 'working', 'you']\n","[[0 1 0 2 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1]\n"," [1 2 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0]\n"," [0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GsTxY-6v8U9O","colab_type":"text"},"source":["## TF-IDF\n","\n","Term frequencyâ€“inverse document frequency is an encoding similar to frequency vector, but represents counts normalized with respect to the rest of the corpus.  We do this with the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."]},{"cell_type":"code","metadata":{"id":"uEghI4J--9J0","colab_type":"code","outputId":"76fc4052-fd7d-491f-ca71-e01ed72d96a5","executionInfo":{"status":"ok","timestamp":1566817391632,"user_tz":-60,"elapsed":469,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":380}},"source":["vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","print(vectorizer.get_feature_names())\n","print(X.toarray())"],"execution_count":35,"outputs":[{"output_type":"stream","text":["['amounts', 'are', 'both', 'can', 'classifier', 'countvectorizer', 'data', 'does', 'for', 'good', 'it', 'job', 'large', 'number', 'of', 'on', 'or', 'organise', 'since', 'so', 'text', 'tfidftransformer', 'tfidfvectorizer', 'the', 'there', 'top', 'train', 'ways', 'we', 'what', 'with', 'working', 'you']\n","[[0.         0.18504333 0.         0.48661948 0.24330974 0.\n","  0.24330974 0.         0.         0.         0.24330974 0.\n","  0.         0.24330974 0.18504333 0.18504333 0.         0.24330974\n","  0.         0.24330974 0.         0.         0.         0.18504333\n","  0.24330974 0.         0.24330974 0.24330974 0.18504333 0.\n","  0.         0.         0.24330974]\n"," [0.25170482 0.38285601 0.25170482 0.         0.         0.19142801\n","  0.         0.         0.25170482 0.25170482 0.         0.25170482\n","  0.25170482 0.         0.19142801 0.         0.25170482 0.\n","  0.25170482 0.         0.25170482 0.         0.19142801 0.19142801\n","  0.         0.         0.         0.         0.19142801 0.\n","  0.19142801 0.25170482 0.        ]\n"," [0.         0.         0.         0.         0.         0.24920411\n","  0.         0.65534691 0.         0.         0.         0.\n","  0.         0.         0.         0.24920411 0.         0.\n","  0.         0.         0.         0.32767345 0.24920411 0.\n","  0.         0.32767345 0.         0.         0.         0.32767345\n","  0.24920411 0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zRGcw6EmBaSO","colab_type":"text"},"source":["Because TF-IDF is a count frequency vector with normalization, it can also be done in two steps by first using the CountVectorizer, and then applying the normalization step with the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n","\n","Verify that it produces the same output."]},{"cell_type":"code","metadata":{"id":"Rk_RKWXU_l9D","colab_type":"code","outputId":"f497fb8c-9a44-4f90-8719-ba9c22e108db","executionInfo":{"status":"ok","timestamp":1566817395536,"user_tz":-60,"elapsed":494,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","transformer = TfidfTransformer()\n","X = transformer.fit_transform(X)\n","X.toarray()"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.18504333, 0.        , 0.48661948, 0.24330974,\n","        0.        , 0.24330974, 0.        , 0.        , 0.        ,\n","        0.24330974, 0.        , 0.        , 0.24330974, 0.18504333,\n","        0.18504333, 0.        , 0.24330974, 0.        , 0.24330974,\n","        0.        , 0.        , 0.        , 0.18504333, 0.24330974,\n","        0.        , 0.24330974, 0.24330974, 0.18504333, 0.        ,\n","        0.        , 0.        , 0.24330974],\n","       [0.25170482, 0.38285601, 0.25170482, 0.        , 0.        ,\n","        0.19142801, 0.        , 0.        , 0.25170482, 0.25170482,\n","        0.        , 0.25170482, 0.25170482, 0.        , 0.19142801,\n","        0.        , 0.25170482, 0.        , 0.25170482, 0.        ,\n","        0.25170482, 0.        , 0.19142801, 0.19142801, 0.        ,\n","        0.        , 0.        , 0.        , 0.19142801, 0.        ,\n","        0.19142801, 0.25170482, 0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.24920411, 0.        , 0.65534691, 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.24920411, 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.32767345, 0.24920411, 0.        , 0.        ,\n","        0.32767345, 0.        , 0.        , 0.        , 0.32767345,\n","        0.24920411, 0.        , 0.        ]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"mKXkljGtB6Gp","colab_type":"text"},"source":["## N-Gram Model\n","\n","So far we have been using the [Bag-of-Words model](https://en.wikipedia.org/wiki/Bag-of-words_model), where each token is one word.  Although this can work in some cases, it doesn't give much context with respect to the rest of the document.\n","\n","N-Gram however stores spacial information by including more than one work per token.  See the examples below.\n","\n","<br>\n","<center>\n","  \n","![Visual representation of unigrams, bigrams, and trigrams](https://www.sqlservercentral.com/wp-content/uploads/legacy/0bf6a2bd621db172dba029ce3c712280a3f6aab3/29444.jpg)\n","  \n","</center>\n","<br>"]},{"cell_type":"code","metadata":{"id":"-E6Hg4iNWZ6i","colab_type":"code","colab":{}},"source":["sequence = [\n","    \"We\",\n","    \"like\",\n","    \"AI\",\n","    \"and\",\n","    \"hope\",\n","    \"you\",\n","    \"like\",\n","    \"it\",\n","    \"too\"\n","]\n","\n","bow = {\n","    \"We\": 1,\n","    \"AI\": 1,\n","    \"and\": 1,\n","    \"hope\": 1,\n","    \"you\": 1,\n","    \"like\": 2,\n","    \"it\": 1,\n","    \"too\": 1\n","}\n","\n","bigram = [\n","    \"We like\",\n","    \"like AI\",\n","    \"AI and\",\n","    \"and hope\",\n","    \"hope you\",\n","    \"you like\",\n","    \"like it\",\n","    \"it too\"\n","]\n","\n","trigram = [\n","    \"We like AI\",\n","    \"like AI and\",\n","    \"AI and hope\",\n","    \"and hope you\",\n","    \"hope you like\",\n","    \"you like it\",\n","    \"like it too\"\n","]\n","\n","ngram = [\n","    \"We like\",\n","    \"We like AI and\",\n","    \"We like AI and hope\",\n","    \"We like AI and hope you\",\n","    \"We like AI and hope you like\",\n","    \"We like AI and hope you like it\",\n","    \"We like AI and hope you like it too\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAyRNAgYj1m2","colab_type":"text"},"source":["# N-Gram Task\n","\n","Write a function that gets a sequence of tokens from the corpus.  The following steps must be included.\n","\n","- Tokenize the text in the corpus. (Hint: Use keras Tokenizer)\n","- For each document in the corpus, get it's sequence and append all n-gram sequences to a list from n=2 up to the sequence length."]},{"cell_type":"code","metadata":{"id":"df8Cco0bxOy0","colab_type":"code","outputId":"ec3bffcb-0a6c-4524-99fc-6e136376b14f","executionInfo":{"status":"ok","timestamp":1566827481057,"user_tz":-60,"elapsed":483,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}},"colab":{"base_uri":"https://localhost:8080/","height":850}},"source":["# TODO: tokenisation\n","t = Tokenizer()\n","\n","\n","# TODO: append n-gram sequences of all documents to list\n","\n","def get_sequence_of_tokens(corpus):\n","  t.fit_on_texts(corpus)\n","  input_sequence = []\n","  num_words = np.array([0])\n","  \n","  for line in corpus:\n","    token_list = t.texts_to_sequences([line])[0]\n","    \n","    for ii in range(1, len(token_list)):\n","      n_gram_sequence = token_list[:ii+1]\n","      input_sequence.append(n_gram_sequence)\n","      \n","    num_words = len(input_sequence) + 1\n","  return input_sequence, num_words\n","\n","  \n","get_sequence_of_tokens(corpus)"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([[12, 2],\n","  [12, 2, 1],\n","  [12, 2, 1, 13],\n","  [12, 2, 1, 13, 3],\n","  [12, 2, 1, 13, 3, 14],\n","  [12, 2, 1, 13, 3, 14, 15],\n","  [12, 2, 1, 13, 3, 14, 15, 4],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4, 19],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4, 19, 1],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4, 19, 1, 20],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4, 19, 1, 20, 7],\n","  [12, 2, 1, 13, 3, 14, 15, 4, 16, 5, 17, 18, 6, 4, 19, 1, 20, 7, 21],\n","  [1, 8],\n","  [1, 8, 22],\n","  [1, 8, 22, 1],\n","  [1, 8, 22, 1, 9],\n","  [1, 8, 22, 1, 9, 2],\n","  [1, 8, 22, 1, 9, 2, 23],\n","  [1, 8, 22, 1, 9, 2, 23, 24],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28, 10],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28, 10, 29],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28, 10, 29, 30],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28, 10, 29, 30, 3],\n","  [1, 8, 22, 1, 9, 2, 23, 24, 25, 5, 26, 27, 6, 2, 28, 10, 29, 30, 3, 31],\n","  [1, 8],\n","  [1, 8, 11],\n","  [1, 8, 11, 32],\n","  [1, 8, 11, 32, 1],\n","  [1, 8, 11, 32, 1, 9],\n","  [1, 8, 11, 32, 1, 9, 11],\n","  [1, 8, 11, 32, 1, 9, 11, 10],\n","  [1, 8, 11, 32, 1, 9, 11, 10, 1],\n","  [1, 8, 11, 32, 1, 9, 11, 10, 1, 33],\n","  [1, 8, 11, 32, 1, 9, 11, 10, 1, 33, 7],\n","  [1, 8, 11, 32, 1, 9, 11, 10, 1, 33, 7, 34]],\n"," 49)"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"code","metadata":{"id":"WpoCJM0FApn7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"61f0d7a6-ce11-4c6f-90e6-12d5fda7634e","executionInfo":{"status":"ok","timestamp":1566825530211,"user_tz":-60,"elapsed":525,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}}},"source":["corpus"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['There are a number of ways you can organise the data so we can train a classifier on it',\n"," 'A TfidfVectorizer or a CountVectorizer are both good for the job since we are working with large amounts of text',\n"," 'A TfidfVectorizer does what a CountVectorizer does with a TfidfTransformer on top.']"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"markdown","metadata":{"id":"mvk0C27RUO6h","colab_type":"text"},"source":["Now alter the same function to return two parameters...\n","1. input_sequences\n","2. total_words\n","\n","You will need to calculate the total number of words within the function and return the two parameters together."]},{"cell_type":"code","metadata":{"id":"BO7i2WyBUoeR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6fd8c421-f49c-4c6a-8395-e7cf03851e21","executionInfo":{"status":"ok","timestamp":1566826897321,"user_tz":-60,"elapsed":499,"user":{"displayName":"Curtis Irvine","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjp-X6R5lEtsA5JrpQ1HoZGUfJ5UJM1aTNko51UNU=s64","userId":"05509147058433014282"}}},"source":["...\n","\n","input_sequences, total_words = get_sequence_of_tokens(corpus)\n","total_words"],"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([50])"]},"metadata":{"tags":[]},"execution_count":94}]}]}